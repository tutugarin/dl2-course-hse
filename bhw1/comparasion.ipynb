{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tutugarin/fairseq_virtualenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-03 20:32:03.809205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 20:32:04.988794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:40<00:00, 13.41s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "transformers.set_seed(42)\n",
    "DEVICE = \"cuda:7\"\n",
    "NUM_SAMPLES = 32\n",
    "CHECKER_TOKENIZER = transformers.AutoTokenizer.from_pretrained(\"berkeley-nest/Starling-LM-7B-alpha\")\n",
    "CHECKER_MODEL = transformers.AutoModelForCausalLM.from_pretrained(\"berkeley-nest/Starling-LM-7B-alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2_generator = transformers.pipeline('text-generation', model='gpt2-xl', device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2_texts = []\n",
    "# for _ in range(NUM_SAMPLES):\n",
    "#     text = gpt2_generator(\"Once upon a time\", max_length=100, num_return_sequences=1)[0][\"generated_text\"]\n",
    "#     gpt2_texts.append(text)\n",
    "# gpt2_texts = [text.replace(\"\\n\", \"\") for text in gpt2_texts]\n",
    "# with open(\"/home/tutugarin/gpt2_tine_stories/gpt2_texts.txt\", 'w') as file:\n",
    "#     for text in gpt2_texts:\n",
    "#         print(text, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, the name \"the Black Lives Matter movement\" would have seemed so alien to the American imagination that the phrase would have been regarded as completely innocuous, or even humorous. But in the past decade, an organized effort from Black youth has made the movement a lightning rod for widespread protests around the country from Ferguson, Mo. to Baltimore, Md. — and one that even Republican presidential candidates have picked up on.( Also on POLITICO: Who\\'s talking about Black lives?\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/tutugarin/gpt2_tine_stories/gpt2_texts.txt\") as file:\n",
    "    gpt2_texts = file.readlines()\n",
    "gpt2_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time there was an old man. his name had a big box of junk in the corner. every day he would take out the box of old things and put them in the corner of his house to make something new and exciting. one morning he was feeling a bit tired so he decided he wanted a nap. so he took a nap and when he woke it was all done, he opened the box and took out all of the old junk. it was so soft, it was like it had been in the box for a very long, curled up in his arms and closed its eyes tight. the man was so tired he fell asleep right there in the box. when he woke up, he was so surprised to find a little puppy inside the box. he smiled and said \"thank you for making me a nap, little pup. i\\'ve been so sleepy and i was so tired!\"\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/tutugarin/gpt2_tine_stories/gen_3.txt\") as file:\n",
    "    ershov_texts = file.readlines()\n",
    "ershov_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppl(model_texts, checker_model=CHECKER_MODEL, checker_tokenizer=CHECKER_TOKENIZER):\n",
    "    checker_model.to(DEVICE)\n",
    "    encodings = checker_tokenizer(\"\\n\\n\".join(model_texts), return_tensors=\"pt\")\n",
    "\n",
    "    max_length = 64\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = checker_model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "            # Multiply it with trg_len to get the summation instead of average.\n",
    "            # We will take average over all the tokens to get the true average\n",
    "            # in the last step of this example.\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / end_loc).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.08954620361328"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl(gpt2_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.377511978149414"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl(ershov_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq_virtualenv_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
